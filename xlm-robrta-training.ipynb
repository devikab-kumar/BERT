{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bQk5S2IC3EZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define your dataset class\n",
        "\n",
        "def encode_emojis(sent):\n",
        "\n",
        "    lst = nltk.word_tokenize(sent)\n",
        "    print(lst)\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=240):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.numerical_labels = self.label_encoder.fit_transform(labels)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.numerical_labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "# Load the pre-trained RoBERTa model and tokenizer\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=4)\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
        "data = pd.read_excel(\"Train.xlsx\")\n",
        "#data['Comments']=data['Comments'].map(encode_emojis)\n",
        "# Define your training dataset and dataloader\n",
        "train_texts = data['Comments']  # Your training text data\n",
        "train_labels = data['Sentiment']  # Your training labels\n",
        "train_dataset = MyDataset(train_texts, train_labels, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "# Define optimizer and training parameters\n",
        "optimizer =torch.optim.AdamW(model.parameters(),lr=2e-6)\n",
        "num_epochs = 3\n",
        "\n",
        "# Fine-tuning loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"xlm-roberta2_model.pt\")\n",
        "tokenizer.save_pretrained(\"xlm-roberta2_model.pt\")"
      ]
    }
  ]
}